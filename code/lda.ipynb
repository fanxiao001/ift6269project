{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy import special\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk import word_tokenize\n",
    "from stop_words import get_stop_words\n",
    "#pip install stop-words\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from gensim import corpora, models\n",
    "import gensim\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer,HashingVectorizer\n",
    "from sklearn.decomposition import NMF, LatentDirichletAllocation\n",
    "from sklearn.datasets import fetch_20newsgroups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, 4), (1, 1), (2, 1), (3, 1), (4, 1), (5, 1), (6, 1), (7, 1), (8, 1), (9, 1), (10, 1), (11, 1), (12, 1), (13, 2), (14, 1), (15, 1), (16, 1), (17, 1), (18, 1), (19, 1), (20, 1), (21, 4), (22, 1), (23, 1), (24, 1), (25, 1), (26, 1), (27, 1), (28, 1), (29, 1), (30, 1), (31, 1), (32, 1), (33, 1), (34, 2), (35, 1), (36, 1), (37, 4), (38, 1), (39, 1), (40, 1), (41, 1), (42, 1), (43, 2), (44, 1), (45, 2), (46, 1), (47, 1), (48, 1), (49, 1), (50, 1), (51, 1), (52, 1), (53, 1), (54, 1), (55, 1), (56, 1), (57, 1), (58, 1), (59, 1), (60, 1), (61, 1), (62, 1)]\n",
      "Dictionary(4581 unique tokens: [u'repris', u'demand', u'hitch', u'four', u'164']...)\n"
     ]
    }
   ],
   "source": [
    "doc_a = \"Brocolli is good to eat. My brother likes to eat good brocolli, but not my mother.\"\n",
    "doc_b = \"My mother spends a lot of time driving my brother around to baseball practice.\"\n",
    "doc_c = \"Some health experts suggest that driving may cause increased tension and blood pressure.\"\n",
    "doc_d = \"I often feel pressure to perform well at school, but my mother never seems to drive my brother to do better.\"\n",
    "doc_e = \"Health professionals say that brocolli is good for your health.\"\n",
    "\n",
    "# compile sample documents into a list\n",
    "#doc_set = [doc_a, doc_b, doc_c, doc_d, doc_e]\n",
    "\n",
    "doc_set = [\n",
    "    'weather: warm, cold, freezing, hot, windy,warm',\n",
    "    'weather: dry, windy, moist, cold, etc',\n",
    "    'freezing means dry and windy',\n",
    "    'sports game, be it basketball, hockey or soccer, I feel better',\n",
    "    'sports can be soothing. hockey, but I like soccer and basketball'\n",
    "]\n",
    "\n",
    "n_samples = 2000\n",
    "n_features = 1000\n",
    "n_components = 10\n",
    "n_top_words = 20\n",
    "\n",
    "dataset = fetch_20newsgroups(shuffle=True, random_state=1,remove=('headers', 'footers', 'quotes'))\n",
    "data_samples = dataset.data[:n_samples]\n",
    "\n",
    "doc_set = data_samples[:100]\n",
    "\n",
    "tokenizer = RegexpTokenizer(r'\\w+')\n",
    "\n",
    "# create English stop words list\n",
    "en_stop = get_stop_words('en')\n",
    "\n",
    "# Create p_stemmer of class PorterStemmer\n",
    "p_stemmer = PorterStemmer()\n",
    "\n",
    "# list for tokenized documents in loop\n",
    "texts = []\n",
    "\n",
    "# loop through document list\n",
    "for i in doc_set:\n",
    "    \n",
    "    # clean and tokenize document string\n",
    "    raw = i.lower()\n",
    "    tokens = tokenizer.tokenize(raw)\n",
    "    #tokens = word_tokenize(raw)\n",
    "\n",
    "    # remove stop words from tokens\n",
    "    stopped_tokens = [i for i in tokens if not i in en_stop]\n",
    "    \n",
    "    # stem tokens\n",
    "    stemmed_tokens = [p_stemmer.stem(i) for i in stopped_tokens]\n",
    "    \n",
    "    # add tokens to list\n",
    "    texts.append(stemmed_tokens)\n",
    "\n",
    "# turn our tokenized documents into a id <-> term dictionary\n",
    "dictionary = corpora.Dictionary(texts)\n",
    "\n",
    "#print(dictionary.token2id)\n",
    "    \n",
    "# convert tokenized documents into a document-term matrix\n",
    "corpus = [dictionary.doc2bow(text) for text in texts]\n",
    "\n",
    "print corpus[0]\n",
    "print dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_sum(log_a,log_b) :\n",
    "    if (log_a < log_b) :\n",
    "        v = log_b+np.log(1 + np.exp(log_a-log_b))\n",
    "    else :\n",
    "        v = log_a+np.log(1 + np.exp(log_b-log_a))\n",
    "    return v\n",
    "\n",
    "\n",
    "class LDA :\n",
    "# find the optimizing values of the vatiational parameters \n",
    "# alpha, beta : hyper-parameters\n",
    "    \n",
    "    def __init__(self,log_file,num_topic,dictionary,doc_term,itrMax = 50, itrEstep = 50, itrMstep = 50, stop = 1e-3):\n",
    "        #self.alpha = np.ones(self.num_topic)\n",
    "        #alpha = np.random.rand(num_topic)\n",
    "        \n",
    "        self.num_topic = num_topic\n",
    "        self.dictionary = dictionary\n",
    "        self.itrMax = itrMax \n",
    "        self.itrEstep = itrEstep\n",
    "        self.itrMstep = itrMstep\n",
    "        self.stop = 1e-3\n",
    "        self.doc_term = doc_term\n",
    "\n",
    "        self.corpus = []\n",
    "        for i in range(self.doc_term.shape[0]) : \n",
    "            w_0 = np.where(self.doc_term[i,:] > 0 )\n",
    "            w_1 = self.doc_term[i,w_0]\n",
    "            words =  np.vstack((w_0,w_1)).T\n",
    "            self.corpus.append(words)\n",
    "\n",
    "        self.num_vocab = len(self.dictionary)      #number of vocabulary\n",
    "        self.num_corpus = len(self.corpus)                   # number of document\n",
    "        \n",
    "        #self.alpha = np.ones(self.num_topic)\n",
    "        self.alpha = np.array(range(self.num_topic)) + 1.0\n",
    "        self.beta = np.zeros((self.num_topic,self.num_vocab))\n",
    "        self.beta[:] = 1.0/self.num_vocab\n",
    "        self.phi_doc = []  # list of size M\n",
    "        self.gamma_doc = []  # list of size M \n",
    "    \n",
    "    def E_step(self,words) : \n",
    "        num_word = words.shape[0]\n",
    "        # initialization\n",
    "        phi = np.zeros((num_word,self.num_topic))  # N*K\n",
    "        phi[:] = 1.0/self.num_topic  \n",
    "        gamma = np.zeros(self.num_topic) \n",
    "        gamma = self.alpha + num_word/self.num_topic\n",
    "\n",
    "        converged_phi = 1.0\n",
    "        converged_gamma = 1.0\n",
    "\n",
    "        itr = 0\n",
    "        while(itr <= self.itrEstep and (converged_phi > self.stop or converged_gamma > self.stop )) : \n",
    "            itr = itr + 1; \n",
    "            phi_new = np.zeros((num_word,self.num_topic))\n",
    "            phisum = np.zeros((num_word))\n",
    "            for n in range(0,num_word) :\n",
    "                for i in range(0,self.num_topic) :\n",
    "                    w_n = words[n,0]\n",
    "                    # overflow because of exp \n",
    "                    #phi_new[n,i] = self.beta[i,w_n]  * np.exp(special.digamma(gamma[i])  -  special.digamma(np.sum(gamma))) \n",
    "                    if self.beta[i,w_n] == 0 : \n",
    "                        self.beta[i,w_n] = np.exp(-100)\n",
    "                    phi_new[n,i] = np.log(self.beta[i,w_n]) + special.digamma(gamma[i])  -  special.digamma(np.sum(gamma))\n",
    "\n",
    "                    if(i==0):\n",
    "                        phisum[n] = phi_new[n,i]\n",
    "                    else : \n",
    "                        phisum[n] = log_sum(phisum[n],phi_new[n,i])\n",
    "\n",
    "                    if (np.isinf(phi_new[n,i])) : \n",
    "                        print \"phi_new is inf\", np.log(self.beta[i,w_n]),special.digamma(gamma[i]) - special.digamma(np.sum(gamma))\n",
    "\n",
    "            #phi_new2 = phi_new / np.sum(phi_new, axis = 1)[:,None]\n",
    "\n",
    "            phi_new2  = np.exp(phi_new - phisum[:,None])\n",
    "\n",
    "            if (np.isnan(phi_new2).any()) :\n",
    "                print phi_new, np.sum(phi_new, axis = 1)\n",
    "            phi_new = phi_new2\n",
    "\n",
    "            gamma_new = self.alpha + np.sum(phi_new, axis = 0)\n",
    "\n",
    "            converged_phi = np.sum(np.abs(phi_new-phi))\n",
    "            converged_gamma = np.sum(np.abs(gamma_new-gamma))\n",
    "\n",
    "            phi = phi_new\n",
    "            gamma = gamma_new\n",
    "            #ll_new = self.log_likelihood([words],self.alpha,self.beta,[phi_new],[gamma_new])\n",
    "            #print \"E step : \", itr, ll_new\n",
    "        return [phi, gamma]\n",
    "        \n",
    "            \n",
    "    def M_step(self) :\n",
    "        self.beta = np.zeros((self.num_topic,self.num_vocab))  #K*V      \n",
    "        for m in range(self.num_corpus) :\n",
    "            for n in range(self.corpus[m].shape[0]) :\n",
    "                for i in range(self.num_topic) :\n",
    "                    j = self.corpus[m][n,0]\n",
    "                    self.beta[i,j] = self.beta[i,j] + self.phi_doc[m][n,i]  * self.corpus[m][n,1]\n",
    "        self.beta = self.beta / np.sum(self.beta,axis = 1)[:,None]\n",
    "\n",
    "        itr = 0\n",
    "        ll_new = self.log_likelihood(self.corpus,self.alpha,self.beta,self.phi_doc,self.gamma_doc)\n",
    "        converged = 1.0\n",
    "\n",
    "        while(itr <= self.itrMstep and converged > self.stop) : \n",
    "\n",
    "            itr = itr + 1\n",
    "            ll_old = ll_new \n",
    "\n",
    "            if(np.isnan(self.alpha).any()) : \n",
    "                self.alpha = self.alpha / 10.0\n",
    "\n",
    "            g = np.zeros(self.num_topic)\n",
    "            g = self.num_corpus * (special.digamma(np.sum(self.alpha)) - special.digamma(self.alpha))  #gradient \n",
    "            for d in range(0,self.num_corpus) : \n",
    "                g = g + special.digamma(self.gamma_doc[d]) - special.digamma(np.sum(self.gamma_doc[d]))\n",
    "\n",
    "            h =  - self.num_corpus * special.polygamma(1,self.alpha) # vector along the diagonal of hessien\n",
    "            z =  special.polygamma(1,np.sum(self.alpha))   # constant\n",
    "            c = np.sum(g/h)/(1.0/z + np.sum(1.0/h))\n",
    "            Hg = (g-c)/h\n",
    "\n",
    "            self.alpha = self.alpha -  Hg\n",
    "            ll_new = self.log_likelihood(self.corpus,self.alpha,self.beta,self.phi_doc,self.gamma_doc)\n",
    "\n",
    "            converged = np.abs(ll_new-ll_old)\n",
    "            #print \"M step : \", itr, ll_new, np.sqrt(np.sum(Hg**2))\n",
    "\n",
    "        return[self.beta,self.alpha]\n",
    "\n",
    "            \n",
    "    def log_likelihood(self,corpus,alpha,beta,phi_doc,gamma_doc) : \n",
    "        l = 0\n",
    "        M = len(phi_doc) # number of document\n",
    "        for m in range(0,M) : \n",
    "            gamma = gamma_doc[m]\n",
    "            phi = phi_doc[m]\n",
    "            words = corpus[m]\n",
    "            len_word = words.shape[0]\n",
    "            term1 = special.gammaln(np.sum(alpha)) - np.sum(special.gammaln(alpha)) \\\n",
    "                    + np.sum((alpha-1)*(special.digamma(gamma)- special.digamma(np.sum(gamma))))\n",
    "            term2 = np.sum(phi * (special.digamma(gamma)- special.digamma(np.sum(gamma)))) \n",
    "\n",
    "            l += term1 + term2\n",
    "\n",
    "            term3 = 0\n",
    "            for n in range(0,len_word) :\n",
    "                w_n = words[n,0]\n",
    "                term3 += np.sum(phi[n,:] * np.log(beta[:,w_n]))\n",
    "            l += term3\n",
    "\n",
    "            term4 = - special.gammaln(np.sum(gamma)) + np.sum(special.gammaln(gamma)) \\\n",
    "                  - np.sum((gamma-1)*(special.digamma(gamma)- special.digamma(np.sum(gamma))))\n",
    "\n",
    "            term5 = 0  #term5 = - np.sum(phi * np.log(phi))\n",
    "            for n in range(0,len_word) :\n",
    "                for i in range(0,num_topic):\n",
    "                    if(phi[n,i] > 0 ) :\n",
    "                        term5 += phi[n,i] * np.log(phi[n,i])\n",
    "\n",
    "            l += term4 + term5\n",
    "\n",
    "            if(np.isnan(term4) or np.isinf(term4)) : \n",
    "                print \"term4 have nan!!!\", gamma \n",
    "\n",
    "            if(np.isnan(term5) or np.isinf(term5)) : \n",
    "                print \"term5 have nan!!!\", phi \n",
    "\n",
    "        return l\n",
    "\n",
    "\n",
    "    def train(self) : \n",
    "        \n",
    "        converged = 1.0\n",
    "        itr = 0\n",
    "        ll_new = 0.0\n",
    "        \n",
    "        while(itr <= self.itrMax and converged > 1e-3 ) :\n",
    "            itr = itr + 1\n",
    "            ll_old = ll_new\n",
    "            self.phi_doc = []  # list of size M\n",
    "            self.gamma_doc = []  # list of size M \n",
    "            for m in range(0,len(self.corpus)) : # E step  : \n",
    "                [phi,gamma] = self.E_step(self.corpus[m])\n",
    "                self.phi_doc.append(phi)\n",
    "                self.gamma_doc.append(gamma)   \n",
    "\n",
    "            ## M step\n",
    "            [beta,alpha] = self.M_step()\n",
    "            ll_new = self.log_likelihood(self.corpus,self.alpha,self.beta,self.phi_doc,self.gamma_doc)\n",
    "            converged = np.abs(ll_new - ll_old)\n",
    "\n",
    "            print \" EM step iteration \" , itr , ll_new\n",
    "            log_string = \" EM step iteration \" + str(itr) +\" : \"+ str(ll_new) + \"\\n\"\n",
    "            log_file.write(log_string)\n",
    "\n",
    "        return [ll_new, self.alpha, self.beta,self.phi_doc,self.gamma_doc]\n",
    "  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, u'0.010*\"s\" + 0.009*\"t\" + 0.008*\"b\"'), (1, u'0.013*\"s\" + 0.008*\"israel\" + 0.008*\"will\"')]\n",
      "359.13420537 4.60016185274\n",
      "4.60016185274 4.60016185274\n"
     ]
    }
   ],
   "source": [
    "# generate LDA model\n",
    "ldamodel = gensim.models.ldamodel.LdaModel(corpus, num_topics=2, id2word = dictionary, passes=20)\n",
    "\n",
    "print(ldamodel.print_topics(num_topics=2, num_words=3))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000\n",
      "(2000, 1000)\n",
      " EM step iteration  1 -566977.909174\n",
      " EM step iteration  2 -566461.063043\n",
      " EM step iteration  3 -566116.269805\n",
      " EM step iteration  4 -565872.35171\n",
      " EM step iteration  5 -565691.985544\n",
      " EM step iteration  6 -565553.791208\n",
      " EM step iteration  7 -565444.76983\n",
      " EM step iteration  8 -565356.761547\n",
      " EM step iteration  9 -565284.361392\n",
      " EM step iteration  10 -565223.816992\n",
      " EM step iteration  11 -565172.511024\n",
      " EM step iteration  12 -565128.528414\n",
      " EM step iteration  13 -565090.450366\n",
      " EM step iteration  14 -565057.228746\n",
      " EM step iteration  15 -565027.997367\n",
      " EM step iteration  16 -565002.112739\n",
      " EM step iteration  17 -564979.046699\n"
     ]
    }
   ],
   "source": [
    "doc_set = [\n",
    "    'weather: warm, cold, freezing, hot, windy, warm',\n",
    "    'weather: dry, windy, moist, cold, etc',\n",
    "    'freezing means dry and windy',\n",
    "    'sports game, be it basketball, hockey or soccer, I feel better, hockey',\n",
    "    'sports can be soothing. hockey, but I like soccer and basketball'\n",
    "]\n",
    "\n",
    "log_file = open(\"error_loss.txt\", \"w\")\n",
    "    \n",
    "n_features = 1000\n",
    "n_components = 10\n",
    "n_top_words = 20\n",
    "n_samples = 2000\n",
    "\n",
    "dataset = fetch_20newsgroups(shuffle=True, random_state=1,remove=('headers', 'footers', 'quotes'))\n",
    "data_samples = dataset.data[:n_samples]\n",
    "doc_set = data_samples[:n_samples]\n",
    "\n",
    "tf_vectorizer = CountVectorizer(max_df=0.95, min_df=2, max_features=n_features,stop_words='english')\n",
    "tf = tf_vectorizer.fit_transform(doc_set)\n",
    "dictionary =  tf_vectorizer.vocabulary_\n",
    "doc_term = tf.toarray()\n",
    "\n",
    "print len(dictionary)\n",
    "print doc_term.shape\n",
    "\n",
    "id2token = {}\n",
    "for token in dictionary.keys() :\n",
    "    id2token[dictionary[token]] = token\n",
    "\n",
    "num_topic = 10\n",
    "num_w = 20\n",
    "\n",
    "model = LDA(log_file,num_topic,dictionary,doc_term,itrMax=100)\n",
    "[ll,alpha, beta,phi_doc,gamma_doc] = model.train()\n",
    "\n",
    "weights = -np.sort(-beta,axis = 1)[:,:num_w]\n",
    "res = np.argsort(-beta,axis = 1)[:,:num_w]\n",
    "\n",
    "print weights\n",
    "\n",
    "for i in range(0,res.shape[0]) :\n",
    "    topic = {}\n",
    "    for j in range(0,res.shape[1]) : \n",
    "        topic[id2token[res[i,j]]]= round(weights[i,j],2)  \n",
    "    print topic.keys()\n",
    "    string =\"topic \" + str(i) + \" : \" +  \";\".join(str(value) for value in topic) + \"\\n\"\n",
    "    log_file.write(string)\n",
    "    \n",
    "    \n",
    "# probability of each tropic of each document\n",
    "for i in range(len(gamma_doc)) :\n",
    "    theta = np.exp(special.digamma(gamma_doc[i]) - special.digamma(np.sum(gamma_doc[i])))\n",
    "    string =\"doc \" + str(i) + \" : \" + \";\".join(str(value) for value in theta) + \"\\n\"\n",
    "    log_file.write(string)\n",
    "    #print theta\n",
    "\n",
    "log_file.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
