{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy import special\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk import word_tokenize\n",
    "from stop_words import get_stop_words\n",
    "#pip install stop-words\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from gensim import corpora, models\n",
    "import gensim\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer,HashingVectorizer\n",
    "from sklearn.decomposition import NMF, LatentDirichletAllocation\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, 4), (1, 1), (2, 1), (3, 1), (4, 1), (5, 1), (6, 1), (7, 1), (8, 1), (9, 1), (10, 1), (11, 1), (12, 1), (13, 2), (14, 1), (15, 1), (16, 1), (17, 1), (18, 1), (19, 1), (20, 1), (21, 4), (22, 1), (23, 1), (24, 1), (25, 1), (26, 1), (27, 1), (28, 1), (29, 1), (30, 1), (31, 1), (32, 1), (33, 1), (34, 2), (35, 1), (36, 1), (37, 4), (38, 1), (39, 1), (40, 1), (41, 1), (42, 1), (43, 2), (44, 1), (45, 2), (46, 1), (47, 1), (48, 1), (49, 1), (50, 1), (51, 1), (52, 1), (53, 1), (54, 1), (55, 1), (56, 1), (57, 1), (58, 1), (59, 1), (60, 1), (61, 1), (62, 1)]\n",
      "Dictionary(4581 unique tokens: [u'repris', u'demand', u'hitch', u'four', u'164']...)\n"
     ]
    }
   ],
   "source": [
    "doc_a = \"Brocolli is good to eat. My brother likes to eat good brocolli, but not my mother.\"\n",
    "doc_b = \"My mother spends a lot of time driving my brother around to baseball practice.\"\n",
    "doc_c = \"Some health experts suggest that driving may cause increased tension and blood pressure.\"\n",
    "doc_d = \"I often feel pressure to perform well at school, but my mother never seems to drive my brother to do better.\"\n",
    "doc_e = \"Health professionals say that brocolli is good for your health.\"\n",
    "\n",
    "# compile sample documents into a list\n",
    "#doc_set = [doc_a, doc_b, doc_c, doc_d, doc_e]\n",
    "\n",
    "doc_set = [\n",
    "    'weather: warm, cold, freezing, hot, windy,warm',\n",
    "    'weather: dry, windy, moist, cold, etc',\n",
    "    'freezing means dry and windy',\n",
    "    'sports game, be it basketball, hockey or soccer, I feel better',\n",
    "    'sports can be soothing. hockey, but I like soccer and basketball'\n",
    "]\n",
    "\n",
    "n_samples = 2000\n",
    "n_features = 1000\n",
    "n_components = 10\n",
    "n_top_words = 20\n",
    "\n",
    "dataset = fetch_20newsgroups(shuffle=True, random_state=1,remove=('headers', 'footers', 'quotes'))\n",
    "data_samples = dataset.data[:n_samples]\n",
    "\n",
    "doc_set = data_samples[:100]\n",
    "\n",
    "tokenizer = RegexpTokenizer(r'\\w+')\n",
    "\n",
    "# create English stop words list\n",
    "en_stop = get_stop_words('en')\n",
    "\n",
    "# Create p_stemmer of class PorterStemmer\n",
    "p_stemmer = PorterStemmer()\n",
    "\n",
    "# list for tokenized documents in loop\n",
    "texts = []\n",
    "\n",
    "# loop through document list\n",
    "for i in doc_set:\n",
    "    \n",
    "    # clean and tokenize document string\n",
    "    raw = i.lower()\n",
    "    tokens = tokenizer.tokenize(raw)\n",
    "    #tokens = word_tokenize(raw)\n",
    "\n",
    "    # remove stop words from tokens\n",
    "    stopped_tokens = [i for i in tokens if not i in en_stop]\n",
    "    \n",
    "    # stem tokens\n",
    "    stemmed_tokens = [p_stemmer.stem(i) for i in stopped_tokens]\n",
    "    \n",
    "    # add tokens to list\n",
    "    texts.append(stemmed_tokens)\n",
    "\n",
    "# turn our tokenized documents into a id <-> term dictionary\n",
    "dictionary = corpora.Dictionary(texts)\n",
    "\n",
    "#print(dictionary.token2id)\n",
    "    \n",
    "# convert tokenized documents into a document-term matrix\n",
    "corpus = [dictionary.doc2bow(text) for text in texts]\n",
    "\n",
    "print corpus[0]\n",
    "print dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 421,
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_sum(log_a,log_b) :\n",
    "    if (log_a < log_b) :\n",
    "        v = log_b+np.log(1 + np.exp(log_a-log_b))\n",
    "    else :\n",
    "        v = log_a+np.log(1 + np.exp(log_b-log_a))\n",
    "    return v\n",
    "\n",
    "\n",
    "class LDA :\n",
    "# find the optimizing values of the vatiational parameters \n",
    "# alpha, beta : hyper-parameters\n",
    "    \n",
    "    def __init__(self,log_file,num_topic,dictionary,doc_term,itrMax = 50, itrEstep = 50, itrMstep = 50, stop = 1e-3,random_state=123):\n",
    "        #self.alpha = np.ones(self.num_topic)\n",
    "        #alpha = np.random.rand(num_topic)\n",
    "        \n",
    "        np.random.seed(random_state)\n",
    "        \n",
    "        self.num_topic = num_topic\n",
    "        self.dictionary = dictionary\n",
    "        self.itrMax = itrMax \n",
    "        self.itrEstep = itrEstep\n",
    "        self.itrMstep = itrMstep\n",
    "        self.stop = 1e-3\n",
    "        self.doc_term = doc_term\n",
    "\n",
    "        self.corpus = []\n",
    "        for i in range(self.doc_term.shape[0]) : \n",
    "            w_0 = np.where(self.doc_term[i,:] > 0 )\n",
    "            w_1 = self.doc_term[i,w_0]\n",
    "            words =  np.vstack((w_0,w_1)).T\n",
    "            self.corpus.append(words)\n",
    "\n",
    "        self.num_vocab = len(self.dictionary)      #number of vocabulary\n",
    "        self.num_corpus = len(self.corpus)                   # number of document\n",
    "        \n",
    "        self.alpha = np.random.rand(self.num_topic)\n",
    "        self.beta = np.zeros((self.num_topic,self.num_vocab))\n",
    "        self.beta = np.random.rand(self.num_topic,self.num_vocab)  # k * V\n",
    "        self.beta = self.beta/np.sum(self.beta,axis = 1)[:,None]\n",
    "        self.phi_doc = []  # list of size M\n",
    "        self.gamma_doc = []  # list of size M \n",
    "    \n",
    "    def E_step(self,words) : \n",
    "        num_word = words.shape[0]\n",
    "        # initialization\n",
    "        phi = np.zeros((num_word,self.num_topic))  # N*K\n",
    "        phi[:] = 1.0/self.num_topic  \n",
    "        gamma = np.zeros(self.num_topic) \n",
    "        gamma = self.alpha + num_word/self.num_topic\n",
    "\n",
    "        converged_phi = 1.0\n",
    "        converged_gamma = 1.0\n",
    "\n",
    "        itr = 1\n",
    "        while(itr <= self.itrEstep and (converged_phi > self.stop or converged_gamma > self.stop )) : \n",
    "            \n",
    "            phi_new = np.zeros((num_word,self.num_topic))\n",
    "            phisum = np.zeros((num_word))\n",
    "            for n in range(0,num_word) :\n",
    "                for i in range(0,self.num_topic) :\n",
    "                    w_n = words[n,0]\n",
    "                    # overflow because of exp \n",
    "                    #phi_new[n,i] = self.beta[i,w_n]  * np.exp(special.digamma(gamma[i])  -  special.digamma(np.sum(gamma))) \n",
    "                    if self.beta[i,w_n] == 0 : \n",
    "                        self.beta[i,w_n] = np.exp(-100)\n",
    "                    phi_new[n,i] = np.log(self.beta[i,w_n]*words[n,1]) + special.digamma(gamma[i])  -  special.digamma(np.sum(gamma))\n",
    "\n",
    "                    if(i==0):\n",
    "                        phisum[n] = phi_new[n,i]\n",
    "                    else : \n",
    "                        phisum[n] = log_sum(phisum[n],phi_new[n,i])\n",
    "\n",
    "                    if (np.isinf(phi_new[n,i])) : \n",
    "                        print \"phi_new is inf\", np.log(self.beta[i,w_n]),special.digamma(gamma[i]) - special.digamma(np.sum(gamma))\n",
    "\n",
    "            #phi_new2 = phi_new / np.sum(phi_new, axis = 1)[:,None]\n",
    "\n",
    "            phi_new2  = np.exp(phi_new - phisum[:,None])\n",
    "\n",
    "            if (np.isnan(phi_new2).any()) :\n",
    "                print phi_new, np.sum(phi_new, axis = 1)\n",
    "            phi_new = phi_new2\n",
    "\n",
    "            gamma_new = self.alpha + np.sum(phi_new, axis = 0)\n",
    "\n",
    "            converged_phi = np.sum(np.abs(phi_new-phi))\n",
    "            converged_gamma = np.sum(np.abs(gamma_new-gamma))\n",
    "\n",
    "            phi = phi_new\n",
    "            gamma = gamma_new\n",
    "            #ll_new = self.log_likelihood([words],self.alpha,self.beta,[phi_new],[gamma_new])\n",
    "           # print \"E step : \", itr, ll_new\n",
    "            \n",
    "            itr = itr + 1\n",
    "            \n",
    "        return [phi, gamma]\n",
    "        \n",
    "            \n",
    "    def M_step(self) :\n",
    "        beta = np.zeros((self.num_topic,self.num_vocab))  #K*V      \n",
    "        for m in range(self.num_corpus) :\n",
    "            for n in range(self.corpus[m].shape[0]) :\n",
    "                for i in range(self.num_topic) :\n",
    "                    j = self.corpus[m][n,0]\n",
    "                    beta[i,j] += self.phi_doc[m][n,i]  * self.corpus[m][n,1]\n",
    "        beta = beta / np.sum(beta,axis = 1)[:,None]\n",
    "\n",
    "        itr = 1\n",
    "        alpha = self.alpha\n",
    "        converged = 1.0\n",
    "\n",
    "        while(itr <= self.itrMstep and converged > self.stop) : \n",
    "\n",
    "            if(np.isnan(alpha).any()) : \n",
    "                alpha = alpha / 10.0\n",
    "\n",
    "            g = np.zeros(self.num_topic)\n",
    "            g = self.num_corpus * (special.digamma(np.sum(alpha)) - special.digamma(alpha))  #gradient \n",
    "            for d in range(0,self.num_corpus) : \n",
    "                g += special.digamma(self.gamma_doc[d]) - special.digamma(np.sum(self.gamma_doc[d]))\n",
    "\n",
    "            h =  - self.num_corpus * special.polygamma(1,alpha) # vector along the diagonal of hessien\n",
    "            z =   special.polygamma(1,np.sum(alpha))   # constant\n",
    "            c = np.sum(g/h)/(1.0/z + np.sum(1.0/h))\n",
    "            Hg = (g-c)/h\n",
    "\n",
    "            alpha = alpha -  Hg\n",
    "\n",
    "            converged = np.sum(np.abs(Hg))\n",
    "            #print \"M step : \", itr, ll_new, np.sqrt(np.sum(Hg**2))\n",
    "            \n",
    "            itr = itr + 1\n",
    "\n",
    "        return[beta,alpha]\n",
    "\n",
    "    def perplexity(self,corpus) :\n",
    "        score = self.log_likelihood(corpus,self.alpha,self.beta,self.phi_doc,self.gamma_doc)\n",
    "        num = 0\n",
    "        for i in range(len(corpus)) :\n",
    "            num += np.sum(corpus[i][:,1])\n",
    "        return np.exp(-score/num)\n",
    "    \n",
    "    def perplexity_score(self,corpus,score) :\n",
    "        num = 0\n",
    "        for i in range(len(corpus)) :\n",
    "            num += np.sum(corpus[i][:,1])\n",
    "        print num\n",
    "        return np.exp(-score/num)\n",
    "            \n",
    "    def log_likelihood(self,corpus,alpha,beta,phi_doc,gamma_doc) : \n",
    "        l = 0\n",
    "        M = len(phi_doc) # number of document\n",
    "        for m in range(0,M) : \n",
    "            gamma = gamma_doc[m]\n",
    "            phi = phi_doc[m]\n",
    "            words = corpus[m]\n",
    "            len_word = words.shape[0]\n",
    "            term1 = special.gammaln(np.sum(alpha)) - np.sum(special.gammaln(alpha)) \\\n",
    "                    + np.sum((alpha-1)*(special.digamma(gamma)- special.digamma(np.sum(gamma))))\n",
    "            term2 = np.sum(phi * (special.digamma(gamma)- special.digamma(np.sum(gamma)))) \n",
    "\n",
    "            l += term1 + term2\n",
    "\n",
    "            term3 = 0\n",
    "            for n in range(0,len_word) :\n",
    "                w_n = words[n,0]\n",
    "                term3 += np.sum(phi[n,:] * np.log(beta[:,w_n])*words[n,1])\n",
    "            l += term3\n",
    "\n",
    "            term4 = - special.gammaln(np.sum(gamma)) + np.sum(special.gammaln(gamma)) \\\n",
    "                  - np.sum((gamma-1)*(special.digamma(gamma)- special.digamma(np.sum(gamma))))\n",
    "\n",
    "            term5 = 0  #term5 = - np.sum(phi * np.log(phi))\n",
    "            for n in range(0,len_word) :\n",
    "                for i in range(0,num_topic):\n",
    "                    if(phi[n,i] > 0 ) :\n",
    "                        term5 -= phi[n,i] * np.log(phi[n,i])\n",
    "\n",
    "            l += term4 + term5\n",
    "\n",
    "            if(np.isnan(term4) or np.isinf(term4)) : \n",
    "                print \"term4 have nan!!!\", gamma \n",
    "\n",
    "            if(np.isnan(term5) or np.isinf(term5)) : \n",
    "                print \"term5 have nan!!!\", phi \n",
    "                \n",
    "            #print m, term1+term4,term2, term3+term5\n",
    "\n",
    "        return l\n",
    "\n",
    "\n",
    "    def train(self) : \n",
    "        \n",
    "        converged = 1.0\n",
    "        itr = 1\n",
    "        ll_new = 0.0\n",
    "        \n",
    "        while(itr <= self.itrMax and converged > 1e-3 ) :\n",
    "            ll_old = ll_new\n",
    "            self.phi_doc = []  # list of size M\n",
    "            self.gamma_doc = []  # list of size M \n",
    "            for m in range(0,len(self.corpus)) : # E step  : \n",
    "                [phi,gamma] = self.E_step(self.corpus[m])\n",
    "                self.phi_doc.append(phi)\n",
    "                self.gamma_doc.append(gamma)   \n",
    "\n",
    "            ## M step\n",
    "            [self.beta,self.alpha] = self.M_step()\n",
    "            ll_new = self.log_likelihood(self.corpus,self.alpha,self.beta,self.phi_doc,self.gamma_doc)\n",
    "            perplexity = self.perplexity_score(self.corpus,ll_new)\n",
    "            converged = np.abs(ll_new - ll_old)\n",
    "\n",
    "            print \" EM step iteration \" , itr , ll_new , \"perplexity : \", perplexity\n",
    "            log_string = \" EM step iteration \" + str(itr) +\" : \"+ str(ll_new) + \"  perplexity : \" + str(perplexity) + \"\\n\"\n",
    "            log_file.write(log_string)\n",
    "            \n",
    "            itr = itr + 1\n",
    "\n",
    "        return [ll_new, self.alpha, self.beta,self.phi_doc,self.gamma_doc]\n",
    "    \n",
    "    def predict(self,corpus) :\n",
    "        corpus_test = []\n",
    "        for i in range(corpus.shape[0]) : \n",
    "            w_0 = np.where(corpus[i,:] > 0 )\n",
    "            w_1 = corpus[i,w_0]\n",
    "            words =  np.vstack((w_0,w_1)).T\n",
    "            corpus_test.append(words)\n",
    "        phi_doc = []\n",
    "        gamma_doc = []\n",
    "        for m in range(len(corpus_test)) : # E step  : \n",
    "            [phi,gamma] = self.E_step(corpus_test[m])\n",
    "            phi_doc.append(phi)\n",
    "            gamma_doc.append(gamma)  \n",
    "        ll = self.log_likelihood(corpus_test,self.alpha,self.beta,phi_doc,gamma_doc)\n",
    "        perplexity = self.perplexity_score(corpus_test,ll)\n",
    "        print \"log-likelihood : \", ll, \" perplexity : \", perplexity\n",
    "        return [phi_doc,gamma_doc]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, u'0.010*\"s\" + 0.009*\"t\" + 0.008*\"b\"'), (1, u'0.013*\"s\" + 0.008*\"israel\" + 0.008*\"will\"')]\n",
      "359.13420537 4.60016185274\n",
      "4.60016185274 4.60016185274\n"
     ]
    }
   ],
   "source": [
    "# generate LDA model\n",
    "ldamodel = gensim.models.ldamodel.LdaModel(corpus, num_topics=2, id2word = dictionary, passes=20)\n",
    "\n",
    "print(ldamodel.print_topics(num_topics=2, num_words=3))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 424,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "300\n",
      "(30, 300)\n",
      "1772\n",
      " EM step iteration  1 -9048.58496444 perplexity :  165.079128151\n",
      "1772\n",
      " EM step iteration  2 -8700.21916484 perplexity :  135.616388651\n",
      "1772\n",
      " EM step iteration  3 -8490.06310617 perplexity :  120.449671433\n",
      "1772\n",
      " EM step iteration  4 -8281.0172332 perplexity :  107.046175662\n",
      "1772\n",
      " EM step iteration  5 -8105.0370568 perplexity :  96.926084994\n",
      "1772\n",
      " EM step iteration  6 -7957.70104298 perplexity :  89.1929448627\n",
      "1772\n",
      " EM step iteration  7 -7847.22704736 perplexity :  83.8020693983\n",
      "1772\n",
      " EM step iteration  8 -7764.61654711 perplexity :  79.9848930957\n",
      "1772\n",
      " EM step iteration  9 -7704.77898309 perplexity :  77.3290275759\n",
      "1772\n",
      " EM step iteration  10 -7659.11848931 perplexity :  75.3618840438\n",
      "[[ 0.1274135   0.03813459  0.02381643]\n",
      " [ 0.06815075  0.03634701  0.03484215]\n",
      " [ 0.12500091  0.12500087  0.12500086]\n",
      " [ 0.12111217  0.0781414   0.05931982]\n",
      " [ 0.18161923  0.06664197  0.05051347]\n",
      " [ 0.04250241  0.03672775  0.03462245]\n",
      " [ 0.12855339  0.07828168  0.05001633]\n",
      " [ 0.09245433  0.03900805  0.03783509]]\n",
      "[u'info', u'com', u'edu']\n",
      "[u'keys', u'use', u'key']\n",
      "[u'252', u'voice', u'political']\n",
      "[u'mail', u'file', u'edu']\n",
      "[u'24', u'gm', u'st']\n",
      "[u'test', u'source', u'number']\n",
      "[u'mail', u'file', u'edu']\n",
      "[u'pc', u'line', u'edu']\n"
     ]
    }
   ],
   "source": [
    "doc_set = [\n",
    "    'weather: warm, cold, freezing, hot, windy, warm',\n",
    "    'weather: dry, windy, moist, cold, etc',\n",
    "    'freezing means dry and windy',\n",
    "    'sports game, be it basketball, hockey or soccer, I feel better, hockey',\n",
    "    'sports can be soothing. hockey, but I like soccer and basketball'\n",
    "]\n",
    "\n",
    "log_file = open(\"error_loss.txt\", \"w\")\n",
    "    \n",
    "n_features = 300\n",
    "n_samples = 30\n",
    "num_topic = 8\n",
    "num_w = 3\n",
    "\n",
    "dataset = fetch_20newsgroups(shuffle=True, random_state=1,remove=('headers', 'footers', 'quotes'))\n",
    "data_samples = dataset.data[:n_samples]\n",
    "doc_set = data_samples[:n_samples]\n",
    "\n",
    "tf_vectorizer = CountVectorizer(max_df=0.95, min_df=2, max_features=n_features,stop_words='english')\n",
    "tf = tf_vectorizer.fit_transform(doc_set)\n",
    "\n",
    "dictionary =  tf_vectorizer.vocabulary_\n",
    "doc_term = tf.toarray()\n",
    "\n",
    "print len(dictionary)\n",
    "print doc_term.shape\n",
    "\n",
    "id2token = {}\n",
    "for token in dictionary.keys() :\n",
    "    id2token[dictionary[token]] = token\n",
    "\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "model = LDA(log_file,num_topic,dictionary,doc_term,itrMax=10)\n",
    "[ll,alpha, beta,phi_doc,gamma_doc] = model.train()\n",
    "\n",
    "end_time = time.time()\n",
    "log_file.write(\"Execution time : \" + str(end_time-start_time) + \"\\n\")\n",
    "\n",
    "weights = -np.sort(-beta,axis = 1)[:,:num_w]\n",
    "res = np.argsort(-beta,axis = 1)[:,:num_w]\n",
    "\n",
    "print weights\n",
    "\n",
    "for i in range(0,res.shape[0]) :\n",
    "    topic = {}\n",
    "    for j in range(0,res.shape[1]) : \n",
    "        topic[id2token[res[i,j]]]= round(weights[i,j],2)  \n",
    "    print topic.keys()\n",
    "    string =\"topic \" + str(i) + \" : \" +  \";\".join(str(value) for value in topic) + \"\\n\"\n",
    "    log_file.write(string)\n",
    "    \n",
    "    \n",
    "# probability of each tropic of each document\n",
    "for i in range(len(gamma_doc)) :\n",
    "    theta = np.exp(special.digamma(gamma_doc[i]) - special.digamma(np.sum(gamma_doc[i])))\n",
    "    string =\"doc \" + str(i) + \" : \" + \";\".join(str(value) for value in theta) + \"\\n\"\n",
    "    log_file.write(string)\n",
    "    #print theta\n",
    "\n",
    "log_file.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 425,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "229\n",
      "log-likelihood :  -1274.71986784  perplexity :  261.507327721\n"
     ]
    }
   ],
   "source": [
    "doc_set_test = dataset.data[n_samples:(10+n_samples)]\n",
    "tf_vectorizer_test = CountVectorizer(max_df=0.95, min_df=2, max_features=n_features,stop_words='english',vocabulary=dictionary)\n",
    "tf_test = tf_vectorizer_test.fit_transform(doc_set_test)\n",
    "corpus_test = tf_test.toarray()\n",
    "#print tf_test[0,:]\n",
    "#print id2token[38]\n",
    "[phi_doc,gamma_doc] = model.predict(corpus_test)\n",
    "\n",
    "#print doc_set_test[0]\n",
    "#print np.exp(special.digamma(gamma_doc[0])- special.digamma(np.sum(gamma_doc[0])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "0 -1.00089839071 -7.40664626527 -47.944721727 -0.504347218797 4.86066237576\n",
    "1 -0.752551094287 -8.84349522294 -65.8330883221 -0.774439595332 5.36543215765\n",
    "2 -0.829933982159 -2.46818589875 -12.2494780657 -0.116693827669 1.01506443292\n",
    "3 -0.515254506679 -7.07592730777 -57.8961527879 -0.881577416936 4.54831330792\n",
    "4 4.86240533432 -0.190629139562 -49.4957379442 -5.49418105512 0.0158450759776\n",
    "5 -0.401891826492 -6.55810434291 -62.6919484683 -0.952276063099 2.8011056097\n",
    "6 -0.931147750031 -4.37954635805 -23.0870372538 -0.349184053082 1.55746184524\n",
    "7 -0.891797231421 -8.40190277457 -52.8320363128 -0.626364157832 3.86741004482\n",
    "8 4.17057396225 -0.176385545212 -18.8907612441 -4.64398116574 0.00303838888048\n",
    "9 4.5595155182 -0.194514486285 -33.162446068 -5.1287594646 0.0193395594631"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[u'pointer' u'connection' u'long' u'font' u'display' u'try']\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n"
     ]
    }
   ],
   "source": [
    "doc_id = 82\n",
    "word_topic = np.argmax(model.phi_doc[doc_id],axis=1)\n",
    "words = tf[doc_id,:].nonzero()[1]\n",
    "id2token = {v: k for k, v in dictionary.iteritems()}\n",
    "words = np.array([id2token[x] for x in words])\n",
    "\n",
    "for i in range(num_topic) :\n",
    "    print words[word_topic==i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
